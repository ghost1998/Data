{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TimeSeriesinKeras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghost1998/Data/blob/master/TimeSeriesinKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1yl3QEMTIUsE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Method overview : **\n",
        "\n",
        "For this project, I have implemented the below methods\n",
        "1.  LSTM model\n",
        "2. Linear Regression\n",
        "3. Linear Least Squares\n",
        "4. Support Vector machine Regressor\n",
        "5. Random Forest Regressor\n",
        "\n",
        "** LSTM Approach **\n",
        "\n",
        "I used an ensemble of single layered LSTMs without attention. LSTMs are a special kind of RNNs which are capable of rmembering long term dependencies. The input consists of lag variables, output consists future variables. \n",
        "\n",
        "  \n",
        "Lag variables at a moment are the values in last n time stamps. Future variables are the values in the next immediate m future timesteps. \n",
        "\n",
        "Each LSTM takes the n lag values of X and Y  (total 2*n variables ) and predicts  m future values of X and Y (2*m variables). \n",
        "\n",
        "The returns are calculated as 60th future step value - current value. \n",
        "\n",
        "From each neural network, we get one returns value. The final answer is the median of these values. Median is  selected because it is most robust to outliers. \n",
        "\n",
        "** Rest of the Models **\n",
        "\n",
        "All the other models use returns generated from last n time stamps as features\n",
        "\n",
        "ie, t[i th second] -t[i -600 th second] for both xprice and yprice\n",
        "\n",
        "And they predict the returns at that time step\n",
        "\n",
        "ie t[i+60 th second] - t[i]"
      ]
    },
    {
      "metadata": {
        "id": "lz2TXu4lCvkF",
        "colab_type": "code",
        "outputId": "3440b6f3-263a-4b6f-e9ef-bd0e017e1842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "# from pandas.tools.plotting import autocorrelation_plot\n",
        "np.nan_to_num(0)\n",
        "import statsmodels\n",
        "# import statsmodels.api as sm\n",
        "# from statsmodels.tsa.stattools import coint, adfuller\n",
        "from scipy.signal import medfilt\n",
        "import random\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.svm import SVR\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.neighbors import KNeighborsRegressor\n",
        "# import xgboost as xgb\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# !pip install pyramid-arima\n",
        "# from pyramid.arima import auto_arima\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras import optimizers\n",
        "\n",
        "def fillNan(arr):\n",
        "    mask = np.isnan(arr)\n",
        "    idx = np.where(~mask,np.arange(mask.shape[1]),0)\n",
        "    np.maximum.accumulate(idx,axis=1, out=idx)\n",
        "    arr[mask] = arr[np.nonzero(mask)[0], idx[mask]]\n",
        "    arr[np.isnan(arr)] = 0\n",
        "    return arr"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YxkAelmaEW11",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Prepare the data for the neural network. Matrix X contains historical data. y contains future data."
      ]
    },
    {
      "metadata": {
        "id": "wnu3BZ4tEOCr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preparedata(series, lags, steps):\n",
        "  \n",
        "  X1 = pd.DataFrame()\n",
        "  Y1 = pd.DataFrame()\n",
        "  X2 = pd.DataFrame()\n",
        "  Y2 = pd.DataFrame()\n",
        "  for i in range(0, lags+1):\n",
        "    X1[\"xlag_{}\".format(i)] = series.xprice.shift(i)\n",
        "    X2[\"ylag_{}\".format(i)] = series.yprice.shift(i)\n",
        "        \n",
        "  for i in range(1, steps+1):\n",
        "    Y1[\"xstep_{}\".format(i)] = series.xprice.shift(-i)    \n",
        "    Y2[\"ystep_{}\".format(i)] = series.yprice.shift(-i)\n",
        "  \n",
        "  X = np.hstack((fillNan(X1.values), fillNan(X2.values)))\n",
        "  Y = np.hstack((fillNan(Y1.values), fillNan(Y2.values)))\n",
        "  X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "  return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLC0ZXr1Ejcm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Network definition : ** \n",
        "\n",
        "Define the LSTM model. We use adam optimizer."
      ]
    },
    {
      "metadata": {
        "id": "wn32Ee4HEiab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getlstmmodel( lags= 40,steps = 80,  n_batch= 1000):\n",
        "  print(n_batch)\n",
        "  n_neurons = 50\n",
        "  n_cut = 8000\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(n_neurons, batch_input_shape=(n_batch, 1, 2+ 2*lags), stateful=True, dropout = 0.3, recurrent_dropout = 0.1))\n",
        "  model.add(Dense(2*steps))\n",
        "  opt = optimizers.SGD(lr=0.005, decay=1e-6)\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return model\n",
        "\n",
        "def getmodels(num_models=7, lags= 40,steps = 80):\n",
        "  return [getlstmmodel(lags, steps) for i in range(num_models)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0U2RYD6DNVic",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training : ** \n",
        "\n",
        "Here we had to make a decision whether to train all the networks on the same data or to split the data. The former worked slightly better. So the code corresponsing to the later is commented out."
      ]
    },
    {
      "metadata": {
        "id": "xs5w8HsMEif1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainall(models,X, Y,transform, n_batch = 1000,n_cut = 8000, max_epochs = 2000):\n",
        "  num_slices = 5 + 2*(len(models) - 1)\n",
        "  for idx,model in enumerate(models):\n",
        "#     p1 = X.shape[0] * ((1+2*idx - 1)/num_slices)\n",
        "#     p2 = X.shape[0] * ((4+2*idx)/num_slices)\n",
        "#     p3 = X.shape[0] * ((5+2*idx)/num_slices)\n",
        "    p1 = X.shape[0] * (0)\n",
        "    p2 = X.shape[0] * (0.7)\n",
        "    p3 = X.shape[0] * (1)\n",
        "    Xtrain = X[int(p1) : int(p2)]\n",
        "    Xtest = X[int(p2) : int(p3)]\n",
        "    Ytrain = Y[int(p1) : int(p2)]\n",
        "    Ytest = Y[int(p2) : int(p3)]    \n",
        "    trainlstm(model,Xtrain, Ytrain, Xtest, Ytest,transform, n_batch,n_cut, max_epochs )\n",
        " \n",
        "\n",
        "def trainlstm (model,Xtrain, Ytrain, Xtest, Ytest,transform, n_batch=1000,n_cut =8000,max_epochs=2000):\n",
        "  print(\"------------------------------------\")\n",
        "  Xtest = Xtest[: n_batch  * int( (Xtest.shape[0]/n_batch) )]\n",
        "  Ytest = Ytest[ : n_batch  * int( (Ytest.shape[0]/n_batch) )]\n",
        "  \n",
        "  for i in range(max_epochs):\n",
        "    print(\"Epoch \" + str(i+1) + \"/\" + str(max_epochs) )\n",
        "    if(i%10 == 0):\n",
        "      pred = model.predict(Xtest, batch_size=n_batch)\n",
        "      e1 = np.sqrt(np.mean(np.square(pred- Ytest)))\n",
        "      print(\"Test Error : \" + str(e1))\n",
        "      predoriginal = (pred + 1) * (transform['max']- transform['min'])/2 + transform['min']\n",
        "      testoriginal = (Ytest + 1) * (transform['max']- transform['min'])/2 + transform['min']\n",
        "      t1 = (predoriginal[:, 139])\n",
        "      t2 = (testoriginal[:, 139])\n",
        "      e1 = np.sqrt(np.mean(np.square(t1- t2)) )\n",
        "      e2 = r2_score(t1, t2)\n",
        "      print(\"RMS Score: \" + str(e1))\n",
        "      print(\"R2 Score: \" + str(e2))\n",
        "      if(e1<0.08 and e2>0.82):\n",
        "        break\n",
        "      if(e1<0.9 and e2>0.9):\n",
        "        break\n",
        "    \n",
        "      \n",
        "      \n",
        "    st = int(random.random() * (Xtrain.shape[0] - n_cut -2))\n",
        "    ed = st+n_cut\n",
        "    model.fit(Xtrain[st:ed], Ytrain[st:ed], epochs=1, batch_size=n_batch, verbose=1, shuffle=False)\n",
        "    model.reset_states()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QDbHZx8qNspE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Function LSTMmodelEstimate() calles the above functions and preprocesses the data, defines the model(s), trains the model(s) on the preprocessed data. \n",
        "\n",
        "It takes arguments the csv file and number of models we wish to ensemble.\n"
      ]
    },
    {
      "metadata": {
        "id": "YSDj_nXiE1PP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def LSTMmodelEstimate(filename, num_models = 3):\n",
        "  print(filename)\n",
        "  dataset = read_csv(filename, header=0,  index_col=0, squeeze=True)\n",
        "  dataset.index = pd.to_datetime(dataset.index, unit='ms')\n",
        "  originalseries = pd.DataFrame()\n",
        "  originalseries['xprice']  = (dataset.xprice) \n",
        "  originalseries['yprice']  = (dataset.yprice)\n",
        "  series = (2*(originalseries - np.min(originalseries.values))/(np.max(originalseries.values) - np.min(originalseries.values))) - 1\n",
        "  transform = {}\n",
        "  transform['min'] = np.min(originalseries.values)\n",
        "  transform['max'] = np.max(originalseries.values)\n",
        "  lags, steps = (40, 80)\n",
        "  n_batch = 1000\n",
        "  n_cut = 8000\n",
        "  max_epochs = 5000\n",
        "  X, Y = preparedata(series, lags, steps)\n",
        "  models = getmodels(num_models, lags, steps)\n",
        "  trainall(models,X,Y,transform, n_batch , n_cut,  max_epochs)\n",
        "  return models, transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ro73XkO8vsw4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function getclassifier takes the Training data and the method name ie which classifier algorithm we wish to use, trains the classifier and returns the appropriate classifier object."
      ]
    },
    {
      "metadata": {
        "id": "eiJ96TM2kPH1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getclassifier(Atrain,ytrain, method ):\n",
        "\n",
        "\n",
        "    clf = None\n",
        "    \n",
        "    if(method == 'RandomForestRegressor'):\n",
        "      clf = RandomForestRegressor( random_state=0, n_estimators=20)\n",
        "      clf.fit(Atrain, ytrain)\n",
        "    \n",
        "    elif(method == 'SVR'):  \n",
        "      clf = SVR()\n",
        "      clf.fit(Atrain, ytrain)\n",
        "      \n",
        "    elif(method is 'LinearRegression'):\n",
        "      clf = LinearRegression()\n",
        "      clf.fit(Atrain, ytrain)\n",
        "      \n",
        "    elif(method is 'LeastSquares'):\n",
        "      X = np.linalg.lstsq(Atrain, ytrain, rcond=None)[0]\n",
        "      return X\n",
        "      \n",
        "    elif(method == 'xgBoost'):\n",
        "      xgdmat = xgb.DMatrix(data=Atrain,label=ytrain)\n",
        "      params={'objective':'reg:linear','max_depth':25, 'n_estimators' : 250}\n",
        "      clf=xgb.train(params,xgdmat) \n",
        "    else:\n",
        "      print(\"Cannot recognise the method \" + str(method))\n",
        "      return\n",
        "    \n",
        "    return clf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqiyLnPwur3p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Function CLFmodelEstimate() calles the above functions and preprocesses the data, defines the classifier, trains the classifier on the preprocessed data.\n",
        "\n",
        "It takes arguments the csv file and method ie which classifier algorithm we wish to use."
      ]
    },
    {
      "metadata": {
        "id": "aX4YgnoOkcar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clfmodelEstimate(filename, method = 'LinearRegression'):\n",
        "    dataset = read_csv(filename, header=0,  index_col=0, squeeze=True)\n",
        "    dataset.index = pd.to_datetime(dataset.index, unit='ms')\n",
        "    dataset['xreturns'] =  dataset['xprice'].shift(-60) - dataset['xprice']\n",
        "  \n",
        "    series = pd.DataFrame()\n",
        "    series['xprice'] = dataset['xprice'] - dataset['xprice'].shift(60)\n",
        "    series['yprice'] = dataset['yprice'] - dataset['yprice'].shift(60)\n",
        "    lags = 150\n",
        "\n",
        "  \n",
        "  \n",
        "    print(\"Number of lags : \" + str(lags))\n",
        "    lagdata = pd.DataFrame()\n",
        "\n",
        "    for i in range(0, lags+1):\n",
        "        lagdata[\"ylag_{}\".format(i)] = series.yprice.shift(i)\n",
        "        lagdata[\"xlag_{}\".format(i)] = series.xprice.shift(i)\n",
        "\n",
        "    yval = dataset['returns']\n",
        "    ytrain = yval.values\n",
        "\n",
        "    clf = None\n",
        "\n",
        "    A = lagdata.values\n",
        "    A = np.nan_to_num(A)\n",
        "    Atrain = A\n",
        "    \n",
        "    clf = getclassifier(Atrain,ytrain, method)\n",
        "    return clf\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4yP0T5FwUa4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the final function which calls all the above functions when needed. \n",
        "\n",
        "It takes name of the csv file and the method ie ie which  algorithm we wish to use as arguments.\n",
        "\n",
        "It returns parameter dictionary which has all the information for inference."
      ]
    },
    {
      "metadata": {
        "id": "9ZZOUvAmgqCp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modelEstimate(filename, method = 'LinearRegression'):\n",
        "  params = {}\n",
        "  params['method'] = method\n",
        "  if(method == 'LSTM'):\n",
        "    models, transform = LSTMmodelEstimate(filename)\n",
        "    params['models'] = models\n",
        "    params['transform'] = transform\n",
        "  else:\n",
        "    params['clf'] = clfmodelEstimate(filename, method)\n",
        "  return params\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "48aBNiZmPWOc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Call the modelEstimate function"
      ]
    },
    {
      "metadata": {
        "id": "Ots6MnyYPWel",
        "colab_type": "code",
        "outputId": "98cb7aa8-ab65-4138-f0ce-999c5d6f87a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "filename = '/content/gdrive/My Drive/data.csv'\n",
        "params = modelEstimate(filename, method='LinearRegression')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lags : 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y2Pgd5HHlq_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c6aacbfd-cfa8-4138-c329-72b2701b3822"
      },
      "cell_type": "code",
      "source": [
        "params"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
              "          normalize=False), 'method': 'LinearRegression'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "83oSl1QJQeYG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Inference : **\n"
      ]
    },
    {
      "metadata": {
        "id": "iQWQzlGCE1VB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(models, X, Y, n_batch):\n",
        "  pad = np.zeros(((n_batch - len(X)%n_batch  ), X[0].shape[0], X[0].shape[1]))\n",
        "  X_new = np.concatenate((X, pad), axis=0)\n",
        "  stck = []\n",
        "  for i in range(len(models)):\n",
        "    pr = models[i].predict(X_new, batch_size=n_batch)\n",
        "    stck.append(pr)\n",
        "  stacked = np.stack(stck)\n",
        "  Ypredwithpad = np.median(stacked, axis = 0)\n",
        "  Ypred = Ypredwithpad[:X.shape[0]]\n",
        "#   print(mean_squared_error(Y, Ypred))\n",
        "  return Ypred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2tA7rRKE1M6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def LSTMmodelForecast(filename, params):\n",
        "  print(filename)\n",
        "  dataset = read_csv(filename, header=0,  index_col=0, squeeze=True)\n",
        "  dataset.index = pd.to_datetime(dataset.index, unit='ms')\n",
        "  originalseries = pd.DataFrame()\n",
        "  originalseries['xprice']  = dataset.xprice \n",
        "  originalseries['yprice']  = dataset.yprice \n",
        "  \n",
        "  transform = params['transform']\n",
        "  models = params['models']\n",
        "  series = (2*(originalseries - transform['min'])/(transform['max'] - transform['min'])) - 1\n",
        "  lags, steps = (40, 80)\n",
        "  n_batch = 1000\n",
        "  n_cut = 8000\n",
        "  max_epochs = 200\n",
        "  X, Y = preparedata(series, lags, steps)\n",
        "  Ypred = predict(models, X, Y, n_batch)\n",
        "  returns = ((Ypred[:, 139]+1)* (transform['max']- transform['min'])/2 + transform['min'] ) - ((X[:, 0, lags+1] + 1) * (transform['max']- transform['min'])/2 + transform['min'])\n",
        "  return returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s0c94cXFpJkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predictclassifier(Atest,clf, method):\n",
        "\n",
        "    predtest = None\n",
        "    if(method == 'RandomForestRegressor'):\n",
        "      predtest = clf.predict(Atest)\n",
        "    \n",
        "    elif(method == 'SVR'):  \n",
        "      predtest = clf.predict(Atest)\n",
        "      \n",
        "    elif(method is 'LinearRegression'):\n",
        "      predtest = clf.predict(Atest)\n",
        "      \n",
        "    elif(method is 'LeastSquares'):\n",
        "      predtest = np.matmul(Atest, clf)\n",
        "      \n",
        "    elif(method == 'xgBoost'):\n",
        "\n",
        "\n",
        "      mat=xgb.DMatrix(Atest)\n",
        "      predtest=clf.predict(mat)\n",
        "    else:\n",
        "      print(\"Cannot recognise the method \" + str(method))\n",
        "      return\n",
        "    \n",
        "    return predtest\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vIAbkjzcpnez",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clfmodelForecast(filename, params):\n",
        "    dataset = read_csv(filename, header=0,  index_col=0, squeeze=True)\n",
        "    dataset.index = pd.to_datetime(dataset.index, unit='ms')\n",
        "    dataset['xreturns'] =  dataset['xprice'].shift(-60) - dataset['xprice']\n",
        "  \n",
        "    series = pd.DataFrame()\n",
        "    series['xprice'] = dataset['xprice'] - dataset['xprice'].shift(60)\n",
        "    series['yprice'] = dataset['yprice'] - dataset['yprice'].shift(60)\n",
        "    lags = 20\n",
        "\n",
        "  \n",
        "  \n",
        "    print(\"Number of lags : \" + str(lags))\n",
        "    lagdata = pd.DataFrame()\n",
        "\n",
        "    for i in range(0, lags+1):\n",
        "        lagdata[\"ylag_{}\".format(i)] = series.yprice.shift(i)\n",
        "        lagdata[\"xlag_{}\".format(i)] = series.xprice.shift(i)\n",
        "\n",
        "    yval = dataset['returns']\n",
        "    ytest = yval.values\n",
        "\n",
        "    clf = None\n",
        "\n",
        "    A = lagdata.values\n",
        "    A = np.nan_to_num(A)\n",
        "    Atest = A\n",
        "    \n",
        "    method = params['method']\n",
        "    clf = params['clf']\n",
        "    \n",
        "    return predictclassifier(Atest,clf, method)\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hORMVY7wxjzD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The function modelForecast() function takes the filename and preproccesses the data appropriately and then calls predict() function.\n",
        "\n",
        "If the method is LSTM,  it calls LSTMmodelForecast() which runs all the models and takes the median. \n",
        "\n",
        "Else it calls clfmodelForecast() which calls their respective predict functions and infers the data.\n",
        "\n",
        "It returns the returns at all the time points\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gnYl5XkKggon",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modelForecast(filename, params):\n",
        "  if(params['method'] is 'LSTM'):\n",
        "    return LSTMmodelForecast(filename, params)\n",
        "  return clfmodelForecast(filename, params)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggGwQCHuEiYX",
        "colab_type": "code",
        "outputId": "f51ab708-5104-492f-d202-bd879bb5d576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "filename = '/content/gdrive/My Drive/data.csv'\n",
        "returns = modelForecast(filename, params)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lags : 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "05kkPG6QSflJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Winner** : Linear Regression seemed to perform the best out of all on validation data. (with an RMS of 0.017)\n",
        "\n",
        "Also I have avoided using packages like fbProphet, AutoArima, ARIMAX, VARMAX, fbprophet because \n",
        "1. They  are a single line pieces of code. So it is naturally I assumed they are prohibitted\n",
        "2. Besides that they are very slow and have to be fit at each time step which will take very much time\n",
        "\n",
        "**Other Methods and Baselines : **\n",
        "\n",
        "1. Ridge and Lasso Regression\n",
        "2. Seq to Seq model\n",
        "3. Seq to Seq Models with attention\n",
        "\n",
        "** Feature Selection : **\n",
        "1. At each timestamp lags and future values are used for LSM.\n",
        "2. Median features (ie median over last 5 min, 10 min, 20 min, 30 min, 60 min, 120 min, 240 min, 480 min) are experimented with and dropped because they give little to no boost in performance with enormous toll on performance. (In rerospection, approximate median kernals could have been used)\n",
        "3. immediate differences (x[t] - x[t-1]) were experimented with. Though these features pass Dickey-Fuller test and should work better, the LSTMs seemed to perform worse. \n",
        "4. For the rest of the classifiers returns generated from last n time stamps are used as features\n",
        " ie, t[i th second] -t[i -600 th second] for both xprice and yprice"
      ]
    }
  ]
}