{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TimeSeriesinKeras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghost1998/Data/blob/master/TimeSeriesinKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1yl3QEMTIUsE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Method overview : **\n",
        "\n",
        "For this project, I have used an ensemble of single layered LSTMs without attention. LSTMs are a special kind of RNNs which are capable of rmembering long term dependencies. The input consists of lag variables, output consists future variables. \n",
        "\n",
        "Lag variables at a moment are the values in last n time stamps. Future variables are the values in the next immediate m future timesteps. \n",
        "\n",
        "Each LSTM takes the n lag values of X and Y  (total 2*n variables ) and predicts  m future values of X and Y (2*m variables). \n",
        "\n",
        "The returns are calculated as 60th future step value - current value. \n",
        "\n",
        "From each neural network, we get one returns value. The final answer is the median of these values. Median is  selected because it is most robust to outliers. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lz2TXu4lCvkF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "276cc880-c92b-4612-d8d5-2177b9fd4f36"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "# from pandas.tools.plotting import autocorrelation_plot\n",
        "np.nan_to_num(0)\n",
        "import statsmodels\n",
        "# import statsmodels.api as sm\n",
        "# from statsmodels.tsa.stattools import coint, adfuller\n",
        "from scipy.signal import medfilt\n",
        "import random\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.svm import SVR\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.neighbors import KNeighborsRegressor\n",
        "# import xgboost as xgb\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# !pip install pyramid-arima\n",
        "# from pyramid.arima import auto_arima\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras import optimizers\n",
        "\n",
        "def fillNan(arr):\n",
        "    mask = np.isnan(arr)\n",
        "    idx = np.where(~mask,np.arange(mask.shape[1]),0)\n",
        "    np.maximum.accumulate(idx,axis=1, out=idx)\n",
        "    arr[mask] = arr[np.nonzero(mask)[0], idx[mask]]\n",
        "    arr[np.isnan(arr)] = 0\n",
        "    return arr"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YxkAelmaEW11",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Prepare the data for the neural network. Matrix X contains historical data. y contains future data."
      ]
    },
    {
      "metadata": {
        "id": "wnu3BZ4tEOCr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preparedata(series, lags, steps):\n",
        "  \n",
        "  X1 = pd.DataFrame()\n",
        "  Y1 = pd.DataFrame()\n",
        "  X2 = pd.DataFrame()\n",
        "  Y2 = pd.DataFrame()\n",
        "  for i in range(0, lags+1):\n",
        "    X1[\"xlag_{}\".format(i)] = series.xprice.shift(i)\n",
        "    X2[\"ylag_{}\".format(i)] = series.yprice.shift(i)\n",
        "        \n",
        "  for i in range(1, steps+1):\n",
        "    Y1[\"xstep_{}\".format(i)] = series.xprice.shift(-i)    \n",
        "    Y2[\"ystep_{}\".format(i)] = series.yprice.shift(-i)\n",
        "  \n",
        "  X = np.hstack((fillNan(X1.values), fillNan(X2.values)))\n",
        "  Y = np.hstack((fillNan(Y1.values), fillNan(Y2.values)))\n",
        "  X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "  return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLC0ZXr1Ejcm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Network definition : ** \n",
        "\n",
        "Define the LSTM model. We use adam optimizer."
      ]
    },
    {
      "metadata": {
        "id": "wn32Ee4HEiab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getlstmmodel( lags= 40,steps = 80,  n_batch= 1000):\n",
        "  print(n_batch)\n",
        "  n_neurons = 50\n",
        "  n_cut = 8000\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(n_neurons, batch_input_shape=(n_batch, 1, 2+ 2*lags), stateful=True, dropout = 0.3, recurrent_dropout = 0.1))\n",
        "  model.add(Dense(2*steps))\n",
        "  opt = optimizers.SGD(lr=0.005, decay=1e-6)\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  return model\n",
        "\n",
        "def getmodels(num_models=7, lags= 40,steps = 80):\n",
        "  return [getlstmmodel(lags, steps) for i in range(num_models)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0U2RYD6DNVic",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training : ** \n",
        "\n",
        "Here we had to make a decision whether to train all the networks on the same data or to split the data. The former worked slightly better. So the code corresponsing to the later is commented out."
      ]
    },
    {
      "metadata": {
        "id": "xs5w8HsMEif1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainall(models,X, Y,transform, n_batch = 1000,n_cut = 8000, max_epochs = 2000):\n",
        "  num_slices = 5 + 2*(len(models) - 1)\n",
        "  for idx,model in enumerate(models):\n",
        "#     p1 = X.shape[0] * ((1+2*idx - 1)/num_slices)\n",
        "#     p2 = X.shape[0] * ((4+2*idx)/num_slices)\n",
        "#     p3 = X.shape[0] * ((5+2*idx)/num_slices)\n",
        "    p1 = X.shape[0] * (0)\n",
        "    p2 = X.shape[0] * (0.7)\n",
        "    p3 = X.shape[0] * (1)\n",
        "    Xtrain = X[int(p1) : int(p2)]\n",
        "    Xtest = X[int(p2) : int(p3)]\n",
        "    Ytrain = Y[int(p1) : int(p2)]\n",
        "    Ytest = Y[int(p2) : int(p3)]    \n",
        "    trainlstm(model,Xtrain, Ytrain, Xtest, Ytest,transform, n_batch,n_cut, max_epochs )\n",
        " \n",
        "\n",
        "def trainlstm (model,Xtrain, Ytrain, Xtest, Ytest,transform, n_batch=1000,n_cut =8000,max_epochs=2000):\n",
        "  print(\"------------------------------------\")\n",
        "  Xtest = Xtest[: n_batch  * int( (Xtest.shape[0]/n_batch) )]\n",
        "  Ytest = Ytest[ : n_batch  * int( (Ytest.shape[0]/n_batch) )]\n",
        "  \n",
        "  for i in range(max_epochs):\n",
        "    print(\"Epoch \" + str(i+1) + \"/\" + str(max_epochs) )\n",
        "    if(i%10 == 0):\n",
        "      pred = model.predict(Xtest, batch_size=n_batch)\n",
        "      e1 = np.sqrt(np.mean(np.square(pred- Ytest)))\n",
        "      print(\"Test Error : \" + str(e1))\n",
        "      predoriginal = (pred + 1) * (transform['max']- transform['min'])/2 + transform['min']\n",
        "      testoriginal = (Ytest + 1) * (transform['max']- transform['min'])/2 + transform['min']\n",
        "      t1 = (predoriginal[:, 139])\n",
        "      t2 = (testoriginal[:, 139])\n",
        "      e1 = np.sqrt(np.mean(np.square(t1- t2)) )\n",
        "      e2 = r2_score(t1, t2)\n",
        "      print(\"RMS Score: \" + str(e1))\n",
        "      print(\"R2 Score: \" + str(e2))\n",
        "      if(e1<0.08 and e2>0.82):\n",
        "        break\n",
        "      if(e1<0.9 and e2>0.9):\n",
        "        break\n",
        "    \n",
        "      \n",
        "      \n",
        "    st = int(random.random() * (Xtrain.shape[0] - n_cut -2))\n",
        "    ed = st+n_cut\n",
        "    model.fit(Xtrain[st:ed], Ytrain[st:ed], epochs=1, batch_size=n_batch, verbose=1, shuffle=False)\n",
        "    model.reset_states()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QDbHZx8qNspE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Final Function modelEstimate() which calls all the above which takes arguments the csv file and number of models.\n"
      ]
    },
    {
      "metadata": {
        "id": "YSDj_nXiE1PP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modelEstimate(filename, num_models = 1):\n",
        "  print(filename)\n",
        "  dataset = read_csv(filename, header=0,  index_col=0, squeeze=True)\n",
        "  dataset.index = pd.to_datetime(dataset.index, unit='ms')\n",
        "  originalseries = pd.DataFrame()\n",
        "  originalseries['xprice']  = (dataset.xprice) \n",
        "  originalseries['yprice']  = (dataset.yprice)\n",
        "  series = (2*(originalseries - np.min(originalseries.values))/(np.max(originalseries.values) - np.min(originalseries.values))) - 1\n",
        "  transform = {}\n",
        "  transform['min'] = np.min(originalseries.values)\n",
        "  transform['max'] = np.max(originalseries.values)\n",
        "  lags, steps = (40, 80)\n",
        "  n_batch = 1000\n",
        "  n_cut = 8000\n",
        "  max_epochs = 5000\n",
        "  X, Y = preparedata(series, lags, steps)\n",
        "  models = getmodels(num_models, lags, steps)\n",
        "  trainall(models,X,Y,transform, n_batch , n_cut,  max_epochs)\n",
        "  return models, transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "48aBNiZmPWOc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Call the modelEstimate function"
      ]
    },
    {
      "metadata": {
        "id": "Ots6MnyYPWel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5296
        },
        "outputId": "d5aede85-c20e-4374-a619-55a8597fb179"
      },
      "cell_type": "code",
      "source": [
        "filename = '/content/gdrive/My Drive/data.csv'\n",
        "params = modelEstimate(filename)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/data.csv\n",
            "1000\n",
            "------------------------------------\n",
            "Epoch 1/5000\n",
            "Test Error : 0.7416986346314767\n",
            "RMS Score: 18.75873706027011\n",
            "R2 Score: -1432.9687714676631\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 1s 81us/step - loss: 0.3363\n",
            "Epoch 2/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 23us/step - loss: 0.2036\n",
            "Epoch 3/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.2127\n",
            "Epoch 4/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0713\n",
            "Epoch 5/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 24us/step - loss: 0.0481\n",
            "Epoch 6/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 26us/step - loss: 0.0451\n",
            "Epoch 7/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 27us/step - loss: 0.0291\n",
            "Epoch 8/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 24us/step - loss: 0.0226\n",
            "Epoch 9/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 23us/step - loss: 0.0324\n",
            "Epoch 10/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 23us/step - loss: 0.0113\n",
            "Epoch 11/5000\n",
            "Test Error : 0.3153831016519527\n",
            "RMS Score: 8.83481106078795\n",
            "R2 Score: -466.51161013140074\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0149\n",
            "Epoch 12/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0301\n",
            "Epoch 13/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0098\n",
            "Epoch 14/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0152\n",
            "Epoch 15/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0070\n",
            "Epoch 16/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0250\n",
            "Epoch 17/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0107\n",
            "Epoch 18/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0090\n",
            "Epoch 19/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0742\n",
            "Epoch 20/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0642\n",
            "Epoch 21/5000\n",
            "Test Error : 0.13723382560042285\n",
            "RMS Score: 3.5262942328783713\n",
            "R2 Score: -35.12153250862768\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0436\n",
            "Epoch 22/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 19us/step - loss: 0.0150\n",
            "Epoch 23/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0254\n",
            "Epoch 24/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0113\n",
            "Epoch 25/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0185\n",
            "Epoch 26/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0107\n",
            "Epoch 27/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0044\n",
            "Epoch 28/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0110\n",
            "Epoch 29/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0059\n",
            "Epoch 30/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0040\n",
            "Epoch 31/5000\n",
            "Test Error : 0.1373072131048278\n",
            "RMS Score: 3.576301418849628\n",
            "R2 Score: -12.018984008688374\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0399\n",
            "Epoch 32/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0095\n",
            "Epoch 33/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0103\n",
            "Epoch 34/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 23us/step - loss: 0.0055\n",
            "Epoch 35/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0084\n",
            "Epoch 36/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0049\n",
            "Epoch 37/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0086\n",
            "Epoch 38/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0043\n",
            "Epoch 39/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0035\n",
            "Epoch 40/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0056\n",
            "Epoch 41/5000\n",
            "Test Error : 0.0640204404835334\n",
            "RMS Score: 1.0804881804133837\n",
            "R2 Score: 0.6300892921909412\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0100\n",
            "Epoch 42/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0086\n",
            "Epoch 43/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0044\n",
            "Epoch 44/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0035\n",
            "Epoch 45/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0047\n",
            "Epoch 46/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0066\n",
            "Epoch 47/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0047\n",
            "Epoch 48/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0028\n",
            "Epoch 49/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0022\n",
            "Epoch 50/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0037\n",
            "Epoch 51/5000\n",
            "Test Error : 0.07393663088145314\n",
            "RMS Score: 1.1850239294465341\n",
            "R2 Score: 0.6664128746996696\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0035\n",
            "Epoch 52/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0034\n",
            "Epoch 53/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0031\n",
            "Epoch 54/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0029\n",
            "Epoch 55/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0021\n",
            "Epoch 56/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0047\n",
            "Epoch 57/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0031\n",
            "Epoch 58/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0237\n",
            "Epoch 59/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0028\n",
            "Epoch 60/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0034\n",
            "Epoch 61/5000\n",
            "Test Error : 0.2375100857760789\n",
            "RMS Score: 4.179497039279638\n",
            "R2 Score: -0.2512125232456315\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0126\n",
            "Epoch 62/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 19us/step - loss: 0.0141\n",
            "Epoch 63/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0063\n",
            "Epoch 64/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0034\n",
            "Epoch 65/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0051\n",
            "Epoch 66/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0070\n",
            "Epoch 67/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0092\n",
            "Epoch 68/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0044\n",
            "Epoch 69/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0031\n",
            "Epoch 70/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0024\n",
            "Epoch 71/5000\n",
            "Test Error : 0.059076973152510925\n",
            "RMS Score: 0.7593332911290867\n",
            "R2 Score: 0.8945105112356195\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0028\n",
            "Epoch 72/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0040\n",
            "Epoch 73/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0043\n",
            "Epoch 74/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0035\n",
            "Epoch 75/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0025\n",
            "Epoch 76/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0020\n",
            "Epoch 77/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0017\n",
            "Epoch 78/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0028\n",
            "Epoch 79/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0029\n",
            "Epoch 80/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0031\n",
            "Epoch 81/5000\n",
            "Test Error : 0.04877886853661318\n",
            "RMS Score: 1.2754759385982726\n",
            "R2 Score: 0.6487219235414245\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0023\n",
            "Epoch 82/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0033\n",
            "Epoch 83/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0019\n",
            "Epoch 84/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0053\n",
            "Epoch 85/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0064\n",
            "Epoch 86/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0060\n",
            "Epoch 87/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0022\n",
            "Epoch 88/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0021\n",
            "Epoch 89/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.0036\n",
            "Epoch 90/5000\n",
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.0029\n",
            "Epoch 91/5000\n",
            "Test Error : 0.05202682128005393\n",
            "RMS Score: 0.509355514665781\n",
            "R2 Score: 0.9535514929559601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "83oSl1QJQeYG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Inference : **\n",
        "\n",
        "modelForecast() function takes the filename and converts the data into proper format and then calls predict() function which runs all the models and takes the median. "
      ]
    },
    {
      "metadata": {
        "id": "iQWQzlGCE1VB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(models, X, Y, n_batch):\n",
        "  pad = np.zeros(((n_batch - len(X)%n_batch  ), X[0].shape[0], X[0].shape[1]))\n",
        "  X_new = np.concatenate((X, pad), axis=0)\n",
        "  stck = []\n",
        "  for i in range(len(models)):\n",
        "    pr = models[i].predict(X_new, batch_size=n_batch)\n",
        "    stck.append(pr)\n",
        "  stacked = np.stack(stck)\n",
        "  Ypredwithpad = np.median(stacked, axis = 0)\n",
        "  Ypred = Ypredwithpad[:X.shape[0]]\n",
        "#   print(mean_squared_error(Y, Ypred))\n",
        "  return Ypred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2tA7rRKE1M6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modelForecast(filename, parameters):\n",
        "  print(filename)\n",
        "  dataset = read_csv(filename, header=0,  index_col=0, squeeze=True)\n",
        "  dataset.index = pd.to_datetime(dataset.index, unit='ms')\n",
        "  originalseries = pd.DataFrame()\n",
        "  originalseries['xprice']  = dataset.xprice \n",
        "  originalseries['yprice']  = dataset.yprice \n",
        "  \n",
        "  transform = parameters[1]\n",
        "  models = parameters[0]\n",
        "  series = (2*(originalseries - transform['min'])/(transform['max'] - transform['min'])) - 1\n",
        "  lags, steps = (40, 80)\n",
        "  n_batch = 1000\n",
        "  n_cut = 8000\n",
        "  max_epochs = 200\n",
        "  X, Y = preparedata(series, lags, steps)\n",
        "  Ypred = predict(models, X, Y, n_batch)\n",
        "  returns = ((Ypred[:, 139]+1)* (transform['max']- transform['min'])/2 + transform['min'] ) - ((X[:, 0, lags+1] + 1) * (transform['max']- transform['min'])/2 + transform['min'])\n",
        "  return returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggGwQCHuEiYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29e130cd-5870-420b-c78f-e4fbed26e174"
      },
      "cell_type": "code",
      "source": [
        "filename = '/content/gdrive/My Drive/data.csv'\n",
        "returns = modelForecast(filename, params)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "05kkPG6QSflJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Other Methods and Baselines : **\n",
        "1. Ordinary Least Squares\n",
        "2. Linear Regression (Ridge and Lasso)\n",
        "3. Support Vector Machines \n",
        "4. XGBoosting\n",
        "5. Seq to Seq model\n",
        "\n",
        "XGBoost gave similar performance but I chose to go with LSTM because it can be improved upon further using things like attention and Beam search  on Seq to Seq model. \n",
        "\n",
        "Also I have avoided using packages like AutoArima, ARIMAX, VARMAX, fbprophet (which are a sngle line pieces of code)\n",
        "\n",
        "** Feature Selection : **\n",
        "1. At each timestamp lags and future values are used.\n",
        "2. Median features (ie median over last 5 min, 10 min, 20 min, 30 min, 60 min, 120 min, 240 min, 480 min) are experimented with and dropped because they give little to no boost in performance with enormous toll on performance. (In rerospection, approximate median kernals could have been used)\n",
        "3. immediate differences (x[t] - x[t-1]) were experimented with. Though these features pass Dickey-Fuller test and should work better, the LSTMs seemed to perform worse. "
      ]
    }
  ]
}